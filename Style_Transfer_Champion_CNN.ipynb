{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306d3501-b19b-4d98-be8d-14d236ba8567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 0.06\n",
      "Step 100, Loss: 0.03\n",
      "Step 200, Loss: 0.04\n",
      "Step 300, Loss: 0.05\n",
      "Step 400, Loss: 0.05\n",
      "Step 500, Loss: 0.03\n",
      "Step 600, Loss: 0.04\n",
      "Step 700, Loss: 0.04\n",
      "Step 800, Loss: 0.06\n",
      "Step 900, Loss: 0.06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------- Settings ---------\n",
    "content_img_path = \"E:/CV/content.jpg\"\n",
    "style_img_path = \"E:/CV/style.jpg\"\n",
    "output_img_path = \"E:/CV/output.jpg\"\n",
    "device = torch.device(\"cpu\")\n",
    "image_size = 256\n",
    "num_steps = 1000  \n",
    "tv_weight = 1e-6\n",
    "\n",
    "# --------- Preprocessing ---------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x[:3, :, :]),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "def load_image(path):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "def im_convert(tensor):\n",
    "    image = tensor.clone().detach().squeeze(0)\n",
    "    image = image * 0.5 + 0.5  # unnormalize\n",
    "    image = image.clamp(0, 1)\n",
    "    return transforms.ToPILImage()(image)\n",
    "\n",
    "# --------- Slightly Modified CNN ---------\n",
    "class SlightlyModifiedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlightlyModifiedCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=9, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Residual blocks\n",
    "            self._residual_block(128),\n",
    "            self._residual_block(128),\n",
    "            self._residual_block(128),\n",
    "            self._residual_block(128),\n",
    "            self._residual_block(128),\n",
    "            # Upsampling\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, kernel_size=9, padding=4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def _residual_block(self, channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x) * 150 + 127.5\n",
    "\n",
    "# --------- Gram Matrix ---------\n",
    "def gram_matrix(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    features = tensor.view(b * c, h * w)\n",
    "    G = torch.mm(features, features.t())\n",
    "    return G / (b * c * h * w)\n",
    "\n",
    "# --------- Load Images ---------\n",
    "content = load_image(content_img_path)\n",
    "style = load_image(style_img_path)\n",
    "\n",
    "# --------- Extract Features ---------\n",
    "cnn = SlightlyModifiedCNN().to(device).eval()\n",
    "with torch.no_grad():\n",
    "    content_features = cnn(content)\n",
    "    style_features = cnn(style)\n",
    "    style_gram = gram_matrix(style_features)\n",
    "\n",
    "# --------- Optimization ---------\n",
    "target = content.clone().requires_grad_(True)\n",
    "optimizer = optim.Adam([target], lr=0.03)\n",
    "content_weight = 1\n",
    "style_weight = 1e5\n",
    "\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    target_features = cnn(target)\n",
    "\n",
    "    c_loss = torch.mean((target_features - content_features) ** 2)\n",
    "    t_gram = gram_matrix(target_features)\n",
    "    s_loss = torch.mean((t_gram - style_gram) ** 2)\n",
    "\n",
    "    tv_loss = torch.sum(torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:])) + \\\n",
    "              torch.sum(torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :]))\n",
    "\n",
    "    loss = content_weight * c_loss + style_weight * s_loss + tv_weight * tv_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item():.2f}\")\n",
    "\n",
    "# --------- Save Result ---------\n",
    "output = im_convert(target)\n",
    "output.save(output_img_path)\n",
    "output.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c9230-b171-414e-aefd-a0f5acc902bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
